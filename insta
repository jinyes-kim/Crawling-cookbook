from selenium import webdriver as wb
from bs4 import BeautifulSoup
from time import sleep
import csv

# input keyword
kwd = input("keyword: ")


#login info
driver = wb.Chrome('chromedriver.exe')
driver.get('https://www.instagram.com/accounts/login/?source=auth_switcher')
sleep(3)

id = 'tempcrawlingbot'
password = ''
id_input = driver.find_elements_by_css_selector('#loginForm > div > div:nth-child(1) > div > label > input')[0]
id_input.send_keys(id)
password_input = driver.find_elements_by_css_selector('#loginForm > div > div:nth-child(2) > div > label > input')[0]
password_input.send_keys(password)
password_input.submit()
sleep(3)


# target page
url = "https://www.instagram.com/explore/tags/" + str(kwd)
driver.get(url)
sleep(3)


item_link = []
while True:
    html_source = driver.page_source
    soup = BeautifulSoup(html_source, 'html.parser')

    for tmp in soup.find_all(name="div", attrs={"class": "Nnq7C weEfm"}):
        dummy_link = tmp.select('a')
        for dummy in dummy_link:
            item_link.append(dummy.attrs['href'])

    # scroll down
    last_height = driver.execute_script("return document.body.scrollHeight")
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    sleep(5)
    new_height = driver.execute_script("return document.body.scrollHeight")

    if new_height == last_height:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        sleep(5)
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        else:
            last_height = new_height
            continue

print(len(item_link))

# crawling
feed = []
for link in item_link:
    article = 'https://www.instagram.com' + str(link)
    driver.get(article)
    sleep(3)

    html_source = driver.page_source
    soup = BeautifulSoup(html_source, 'html.parser')

    for tmp in soup.find_all(name='div', attrs={"class": "C4VMK"}):
        date_data = soup.find(name='time', attrs={"class": "_1o9PC Nzb55"}).attrs['datetime']
        tmp_list = [date_data]
        for word in tmp:
            tmp_list.append(word.get_text())
        feed.append(tmp_list)

# export csv file
output = open(kwd + '.csv', 'w', encoding='utf-8-sig', newline='')
wr = csv.writer(output)
wr.writerow(['date', 'user_id', 'text', 'like', 'tag'])

for record in feed:
    wr.writerow(record)

output.close()



"""
# add like
pp_list = []
for article in feed:
    tmp = []
    tmp.append(article[0]) # date
    tmp.append(article[2]) # review

    like = article[3]
    if '좋아요' in like:
        num = like.find('좋아요')
        end_num = like.find('개')
        tmp.append(like[num + 3: end_num].strip())
    else:
        tmp.append('0')

    pp_list.append(tmp)



# add hash tag
result_list = []
for tmp in pp_list:
    hash_tag_list = []
    if '#' in tmp[1]:
        tmp_list = tmp[1].split(" ")
        for tag in tmp_list:
            if tag.startswith('#'):
                hash_tag_list.append(tag)

        tmp.append(', '.join(hash_tag_list))

    else:
        tmp.append('null')
    result_list.append(tmp)

del pp_list

eng_result = []
for i in result_list:
    text = re.sub('[^0-9a-zA-Z]', ' ', i[1]).strip()
    tag = re.sub('[^0-9a-zA-Z]', ' ', i[3]).strip()
    if tag != '':
        i[3] = tag
    else:
        i[3] = 'null'

    if text != '':
        i[1] = text
        eng_result.append(i)


del result_list
pprint.pprint(eng_result)
"""
