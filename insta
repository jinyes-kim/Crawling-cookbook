from selenium import webdriver as wb
from bs4 import BeautifulSoup
from time import sleep
import pprint

kwd = input("keyword: ")
url = "https://www.instagram.com/explore/tags/" + str(kwd)
driver = wb.Chrome('chromedriver.exe')
driver.get(url)
sleep(3)

#login

item_link = []


# append #tag link
#while True:
html_source = driver.page_source
soup = BeautifulSoup(html_source, 'html.parser')

for tmp in soup.find_all(name="div", attrs={"class":"Nnq7C weEfm"}):
    title = tmp.select('a')[0]
    item_link.append(title.attrs['href'])



# crawling
feed = []
for link in item_link:
    article = 'https://www.instagram.com' + str(link)
    driver.get(article)
    sleep(3)

    html_source = driver.page_source
    soup = BeautifulSoup(html_source, 'html.parser')

    for tmp in soup.find_all(name='div', attrs={"class": "C4VMK"}):
        date_data = soup.find(name='time', attrs={"class": "_1o9PC Nzb55"}).attrs['datetime']
        tmp_list = [date_data]
        for word in tmp:
            tmp_list.append(word.get_text())
        feed.append(tmp_list)



# add like
pp_list = []
for article in feed:
    tmp = []
    tmp.append(article[0])
    tmp.append(article[2])

    like = article[3]
    if '좋아요' in like:
        tmp.append(like[2:7])
    else:
        tmp.append('좋아요 0')

    pp_list.append(tmp)


# add hash tag
result_list = []
for tmp in pp_list:
    hash_tag_list = []
    if '#' in tmp[0]:
        tmp_list = tmp[0].split(" ")
        for tag in tmp_list:
            if tag.startswith('#'):
                hash_tag_list.append(tag)

        tmp.append(','.join(hash_tag_list))

    else:
        tmp.append('null')

    result_list.append(tmp)

pprint.pprint(result_list)

